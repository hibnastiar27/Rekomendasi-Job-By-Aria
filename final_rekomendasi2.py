# -*- coding: utf-8 -*-
"""Final Rekomendasi2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/186PcZGsawOp-NsiLI2DpKlBhXnwBQKUx

# Import
"""

pip install kagglehub[pandas-datasets]

# Dasar
import kagglehub
from kagglehub import KaggleDatasetAdapter
import pandas as pd
import re

# Modling Dan Evaluasi
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

"""# Load Data"""

# Set the path to the file you'd like to load
file_path = "job_descriptions.csv"

# Load the latest version
df = kagglehub.load_dataset(
  KaggleDatasetAdapter.PANDAS,
  "ravindrasinghrana/job-description-dataset",
  file_path,
  # Provide any additional arguments like
  # sql_query or pandas_kwargs. See the
  # documenation for more information:
  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas
)

"""# EDA

Jumlah Keseluruhan Data
"""

df.info()

"""Fitur yang akan diolah

1. Job Title
2. Job Description
3. skills
4. Responsibilities
5. Qualifications
6. location
7. Work Type

Kolom Unik
"""

len(df['Job Title'].unique())

len(df['skills'].unique())

print(df['Qualifications'].unique()) # Pendidikan
print(len(df['Qualifications'].unique()))

# print(df['Responsibilities'].unique()) # Pengalaman
print(len(df['Responsibilities'].unique()))

print(df['location'].unique()) # Lokasi
print(len(df['location'].unique()))

print(df['Work Type'].unique()) # Jenis Pekerjaan
print(len(df['Work Type'].unique()))

"""## Summary

1. Total data ada 1juta lebih, tetapi yang akan digunakan hanya sample yaitu 10 ribu saja.
2. yang akan digunakan ada 7 yaitu :
  1. Job Title
  2. Job Description
  3. skills
  4. Responsibilities
  5. Qualifications
  6. location
  7. Work Type

# Cleaning

- Filter hanya 7 fitur seperti yang di EDA
- Hilangkan Null Jika ada
- Hilangkan Duplikat jika ada
- Buat jadi sample 10k data

Sampling dulu jadi 10k data
"""

df_sample = df.sample(n=10000, random_state=27)
df_sample.info()

"""7 Fitur"""

df_clean = df_sample.copy()

df_clean = df_clean[['Job Title', 'Job Description', 'skills', 'Responsibilities', 'Qualifications', 'location', 'Work Type']]

df_clean.info()

"""Hapus Duplikat"""

df_clean.duplicated().sum()

df_clean = df_clean.drop_duplicates()
df_clean.duplicated().sum()

df_clean.info()

"""Jika Null Hapus"""

df_clean.isnull().sum()

"""## Sumary

1. 7 Fitur yang digunakan
2. Terdapat Duplicate sebanyak (19) dan dihapus
3. tidak ada data yang null
"""

df_clean.head()

df_clean['Job Title'].value_counts()

"""# Modeling"""

df_modeling = df_clean.copy()

"""## Data  Preprosessing"""

df_modeling = df_modeling.reset_index(drop=True)

df_modeling['combined_text'] = (
  df_modeling['skills'] + ' ' +
  df_modeling['Job Description'] + ' ' +
  df_modeling['Responsibilities'] + ' ' +
  df_modeling['Qualifications'] + ' ' +
  df_modeling['location'] + ' ' +
  df_modeling['Work Type']
)

def clean_text(text):
  text = text.lower()
  text = re.sub(r'\d+', '', text)  # hapus angka
  text = re.sub(r'[^\w\s]', '', text)  # hapus tanda baca
  return text

df_modeling['combined_text'] = df_modeling['combined_text'].apply(clean_text)

"""Ingin prediksi `Work Type`"""

label_encoder = LabelEncoder()
df_modeling['Work Type Encoded'] = label_encoder.fit_transform(df_modeling['Work Type'])

"""## Content Based Filtering"""

# TF-IDF Vectirization
tfidf = TfidfVectorizer(stop_words='english', max_features=10000)
tfidf_matrix = tfidf.fit_transform(df_modeling['combined_text'])

# Cosine Similarty
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

# Lowercase untuk pencarian job title
df_modeling['Job Title Lower'] = df_modeling['Job Title'].str.lower()

# Mapping index berdasarkan job title lowercase
indices = pd.Series(df_modeling.index, index=df_modeling['Job Title Lower']).drop_duplicates()

# Fungsi Rekomendasi
# indices = pd.Series(df_modeling.index, index=df_modeling['Job Title']).drop_duplicates()
def recommend_jobs(job_title, top_n=5):
  # Cari job title yang mengandung kata kunci, dalam df_modeling
  matches = df_modeling[df_modeling['Job Title'].str.lower().str.contains(job_title.lower())]

  if matches.empty:
    return "Job Title tidak ditemukan."

  # Ambil index posisi (0â€“9999) dari hasil match (bukan index asli dari dataset awal)
  idx = matches.index[0]

  # Hitung similarity terhadap semua job
  sim_scores = list(enumerate(cosine_sim[idx]))
  sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

  # Ambil top_n, skip dirinya sendiri (index ke-0)
  sim_scores = sim_scores[1:top_n+1]
  job_indices = [i[0] for i in sim_scores]
  scores = [round(score * 100, 2) for _, score in sim_scores]

  # Ambil data dari df_modeling
  result = df_modeling.iloc[job_indices][['Job Title', 'location', 'Work Type']].copy()
  result['Similarity (%)'] = scores

  return result

df_modeling = df_modeling.reset_index(drop=True)

recommend_jobs('Software Engine')

"""# Evaluation

## Pressision@k
"""

ground_truth = {
  'software engineer': ['software engineer', 'software developer'],
  'data analyst': ['data analyst', 'business analyst'],
  'marketing manager': ['marketing manager', 'digital marketing'],
  'project manager': ['project manager'],
  'ui/ux': ['ui/ux', 'Designer', 'ui/ux designer'],
  'sales': ['sales', 'sales associat']
}

def precision_at_k(recommended, relevant, k=5):
  recommended_titles = recommended['Job Title'].str.lower().tolist()
  relevant_lower = [r.lower() for r in relevant]
  hits = sum(1 for title in recommended_titles if any(rel in title for rel in relevant_lower))
  return hits / k

test_queries = ['software engineer', 'data analyst', 'marketing manager', 'project manager', 'ui/ux', 'data scientist']
precisions = []

for query in test_queries:
  recs = recommend_jobs(query, top_n=5)
  if isinstance(recs, str):
    print(f"Query '{query}' tidak ditemukan.")
    continue
  prec = precision_at_k(recs, ground_truth.get(query, []))
  print(f"Precision@5 untuk '{query}': {prec:.2f}")
  precisions.append(prec)

print(f"Rata-rata Precision@5: {sum(precisions)/len(precisions):.2f}")

"""## Coverage"""

valid = 0
for query in test_queries:
    recs = recommend_jobs(query, top_n=5)
    if not isinstance(recs, str):
        valid += 1
coverage = valid / len(test_queries)
print(f"Coverage: {coverage*100:.2f}%")

"""
# Metrik Evaluasi

1. **Precision\@k**
   Precision\@k mengukur proporsi pekerjaan yang relevan di antara *k* rekomendasi teratas yang diberikan oleh sistem.
   Rumusnya:

   $$\text{Precision@k} = \frac{\text{Jumlah rekomendasi relevan dalam k teratas}}{k}$$

   Metrik ini menilai ketepatan rekomendasi yang disajikan kepada pengguna, dimana nilai lebih tinggi menunjukkan relevansi yang lebih baik.

2. **Coverage**
   Coverage mengukur proporsi query pencarian pekerjaan yang berhasil diberikan rekomendasi oleh sistem dari seluruh query yang diuji.
   Rumusnya:

   $$\text{Coverage} = \frac{\text{Jumlah query dengan rekomendasi valid}}{\text{Total query}} \times 100\%$$

   Metrik ini menilai sejauh mana sistem mampu memberikan rekomendasi untuk berbagai jenis pekerjaan yang dicari pengguna.


   ### Kesimpulan

Model rekomendasi pekerjaan berbasis content-based filtering ini telah berhasil memberikan rekomendasi yang relevan dengan tingkat akurasi tinggi untuk sebagian besar query. Coverage sebesar 83.33% menunjukkan bahwa sistem cukup luas dalam menyediakan rekomendasi untuk berbagai jenis pekerjaan. Meskipun demikian, ada ruang perbaikan terutama pada jenis pekerjaan dengan data lebih terbatas atau query yang kurang tepat, yang dapat menjadi fokus pengembangan selanjutnya untuk meningkatkan cakupan dan akurasi rekomendasi.
"""